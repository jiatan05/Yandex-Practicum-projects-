{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Customers: Leave or Stay - Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an analyst at Beta Bank, the job is to analyze behavior data about customers and their termination of contracts with the bank.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction\n",
    "Presented is an analysis of user behavior data that have terminated their contracts with the bank. \n",
    "\n",
    "###  Goal:\n",
    "This report will focus on developing a model that would predict whether a customer will leave or stay based on various behavior data such as: their credit score, geographical location, Gender, Age, how long they've been with the bank (Tenure), Balance, Number of Products they use, whether they have a credit card, if they are active members, and their Estimated Salary.\n",
    "The final model will have the highest possible F1 score. The threshold for F1 score is set at 0.59.\n",
    "\n",
    "\n",
    "### Stages:\n",
    "This project will consist of the following stages:\n",
    "\n",
    "1. Introduction\n",
    "2. General Information\n",
    "3. Data Preprocessing\n",
    "    1. Missing values\n",
    "    2. Drop columns\n",
    "    3. Dummies values\n",
    "    4. Scaling\n",
    "4. Split Dataset\n",
    "    1. Training\n",
    "    2. Validation\n",
    "    3. Test \n",
    "5. Model Testing with Class Imbalance\n",
    "    1. Decision Tree\n",
    "    2. Random Forest\n",
    "    3. Logistic Regression\n",
    "5. Model Improvement\n",
    "    1. Class weight adjustment\n",
    "    2. Upsampling/downsampling\n",
    "6. Final Test\n",
    "7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's open the dataset and study it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the dataset\n",
    "\n",
    "bank = pd.read_csv('/datasets/Churn.csv')\n",
    "\n",
    "#general information\n",
    "\n",
    "bank.info()\n",
    "\n",
    "#sample of the data\n",
    "\n",
    "bank.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consist of the following columns, by separating them into features and target:\n",
    "\n",
    "**Features**\n",
    "- `RowNumber` — data string index\n",
    "- `CustomerId` — unique customer identifier\n",
    "- `Surname` — surname\n",
    "- `CreditScore` — credit score\n",
    "- `Geography` — country of residence\n",
    "- `Gender` — gender\n",
    "- `Age` — age\n",
    "- `Tenure` — period of maturation for a customer’s fixed deposit (years)\n",
    "- `Balance` — account balance\n",
    "- `NumOfProducts` — number of banking products used by the customer\n",
    "- `HasCrCard` — customer has a credit card\n",
    "- `IsActiveMember` — customer’s activeness\n",
    "- `EstimatedSalary` — estimated salary\n",
    "\n",
    "**Target** \n",
    "\n",
    "- `Exited` — сustomer has left\n",
    "\n",
    "Our target will the **'Exited'** column, since the values are 0 and 1, this will be a classification model. \n",
    "\n",
    "There are some missing values in the **'tenure'** column, we should address it. \n",
    "\n",
    "However, not all of the columns in features are useful, we should exclude **'RowNumber', 'CustomerId', and 'Surname'** columns, as they do not contribute to our model. \n",
    "\n",
    "Next, we will need to create some dummies (convert to numerical) for categorical values, such as **'Gender'** and **'Geography'**. \n",
    "\n",
    "Lastly, we need to split our dataset into **train, validation and test** sets following a 3:1:1 ratio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  8.,  7.,  4.,  6.,  3., 10.,  5.,  9.,  0., nan])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check missing values\n",
    "\n",
    "bank['Tenure'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill those missing values with the median to avoid potential bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#fill with median\n",
    "\n",
    "bank['Tenure'] = bank['Tenure'].fillna(bank['Tenure'].median())\n",
    "\n",
    "#check\n",
    "\n",
    "bank.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will exclude **'RowNumber', 'CustomerId', and 'Surname'** columns, as they do not contribute to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CreditScore      10000 non-null  int64  \n",
      " 1   Geography        10000 non-null  object \n",
      " 2   Gender           10000 non-null  object \n",
      " 3   Age              10000 non-null  int64  \n",
      " 4   Tenure           10000 non-null  float64\n",
      " 5   Balance          10000 non-null  float64\n",
      " 6   NumOfProducts    10000 non-null  int64  \n",
      " 7   HasCrCard        10000 non-null  int64  \n",
      " 8   IsActiveMember   10000 non-null  int64  \n",
      " 9   EstimatedSalary  10000 non-null  float64\n",
      " 10  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(6), object(2)\n",
      "memory usage: 859.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# New dataset\n",
    "\n",
    "bank = bank.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1)\n",
    "\n",
    "#check\n",
    "\n",
    "bank.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Categorical column to Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use One-Hot Encoding (ohe) for this, and we will drop the first column (dummy trap) to avoid excessive columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   CreditScore        10000 non-null  int64  \n",
      " 1   Age                10000 non-null  int64  \n",
      " 2   Tenure             10000 non-null  float64\n",
      " 3   Balance            10000 non-null  float64\n",
      " 4   NumOfProducts      10000 non-null  int64  \n",
      " 5   HasCrCard          10000 non-null  int64  \n",
      " 6   IsActiveMember     10000 non-null  int64  \n",
      " 7   EstimatedSalary    10000 non-null  float64\n",
      " 8   Exited             10000 non-null  int64  \n",
      " 9   Geography_Germany  10000 non-null  uint8  \n",
      " 10  Geography_Spain    10000 non-null  uint8  \n",
      " 11  Gender_Male        10000 non-null  uint8  \n",
      "dtypes: float64(3), int64(6), uint8(3)\n",
      "memory usage: 732.5 KB\n"
     ]
    }
   ],
   "source": [
    "#One-Hot Encoding\n",
    "\n",
    "bank_ohe = pd.get_dummies(bank, drop_first = True)\n",
    "\n",
    "#check\n",
    "\n",
    "bank_ohe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that one hot encoding created 3 columns 'Geography_germany', 'Geography_spain' and 'Gender_male'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split our dataset into **train, validation and test** sets following a 3:1:1 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 11) (2000, 11) (2000, 11)\n"
     ]
    }
   ],
   "source": [
    "#target and feature creation\n",
    "\n",
    "target = bank_ohe['Exited']\n",
    "features = bank_ohe.drop(['Exited'],axis = 1)\n",
    "\n",
    "#split the dataset\n",
    "\n",
    "features_train, features_temp, target_train, target_temp = train_test_split(features, target, test_size = 0.4, random_state = 12345)\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(features_temp, target_temp, test_size = 0.5, random_state = 12345)\n",
    "\n",
    "print(features_train.shape,\n",
    "     features_valid.shape,\n",
    "     features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully split our data into train, validation and test sets following a 3:1:1 ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing with Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.7963\n",
       "1    0.2037\n",
       "Name: Exited, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the balance\n",
    "\n",
    "bank_ohe['Exited'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Exited = 0 is significantly more than Exited = 1. Therefore the class is not balanced. But let's test models with class imbalance and check their F1 and AUC-ROC values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test 3 different models: Decision Tree, Random Forest, and the Logistic Regression. Since our target is categorical (0, 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to adjust the hyperparameter: max_depth to decide which will provide the best F1 score & AUC-ROC score. Since we used random_state = 12345 on our dataset split, we will keep this hyperparameter the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth of 1 F1 Score = 0.0 AUC-ROC score = 0.6925565119556736\n",
      "Max depth of 2 F1 Score = 0.5217391304347825 AUC-ROC score = 0.7501814673449512\n",
      "Max depth of 3 F1 Score = 0.4234875444839857 AUC-ROC score = 0.7973440741838507\n",
      "Max depth of 4 F1 Score = 0.5528700906344411 AUC-ROC score = 0.813428129858032\n",
      "Max depth of 5 F1 Score = 0.5406249999999999 AUC-ROC score = 0.8221680508592478\n",
      "Max depth of 6 F1 Score = 0.5696969696969697 AUC-ROC score = 0.8164631712023421\n",
      "Max depth of 7 F1 Score = 0.5320813771517998 AUC-ROC score = 0.8138530658907929\n",
      "Max depth of 8 F1 Score = 0.5454545454545454 AUC-ROC score = 0.8119854644656693\n",
      "Max depth of 9 F1 Score = 0.5633802816901409 AUC-ROC score = 0.7801515554775917\n",
      "Max depth of 10 F1 Score = 0.5413744740532961 AUC-ROC score = 0.766149081472789\n"
     ]
    }
   ],
   "source": [
    "#loop through max_depth 1-10.\n",
    "for depth in range (1,11):\n",
    "    tree_model = DecisionTreeClassifier(random_state = 12345, max_depth = depth)\n",
    "    tree_model.fit(features_train, target_train)\n",
    "    tree_prediction = tree_model.predict(features_valid)\n",
    "    prob_valid = tree_model.predict_proba(features_valid)\n",
    "    prob_1_valid = prob_valid[:,1]\n",
    "    print('Max depth of', depth, 'F1 Score =', f1_score(target_valid, tree_prediction), 'AUC-ROC score =', roc_auc_score(target_valid, prob_1_valid))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that max depth of **6** give a F1 score of ~0.57, AUC-ROC score of ~0.82."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to adjust the hyperparameter: max_depth and n_estimators to decide which will provide the best F1 & AUC_ROC score. Since we used random_state = 12345 on our dataset split, we will keep this hyperparameter the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth of 6 n_estimators= 10 F1 Score = 0.5475409836065573 AUC-ROC score = 0.8417075472268765\n",
      "Max depth of 6 n_estimators= 20 F1 Score = 0.5333333333333333 AUC-ROC score = 0.8490199251144757\n",
      "Max depth of 6 n_estimators= 30 F1 Score = 0.5306799336650083 AUC-ROC score = 0.8486993328050617\n",
      "Max depth of 6 n_estimators= 40 F1 Score = 0.53 AUC-ROC score = 0.8504580538232145\n",
      "Max depth of 6 n_estimators= 50 F1 Score = 0.5348837209302326 AUC-ROC score = 0.8499771653590935\n",
      "Max depth of 6 n_estimators= 60 F1 Score = 0.5460526315789473 AUC-ROC score = 0.8510833600493591\n",
      "Max depth of 6 n_estimators= 70 F1 Score = 0.5339966832504146 AUC-ROC score = 0.8505873493064923\n",
      "Max depth of 6 n_estimators= 80 F1 Score = 0.533112582781457 AUC-ROC score = 0.8512693640779341\n",
      "Max depth of 6 n_estimators= 90 F1 Score = 0.5257903494176372 AUC-ROC score = 0.8515476140068593\n",
      "Max depth of 6 n_estimators= 100 F1 Score = 0.5266666666666666 AUC-ROC score = 0.8522371899176744\n",
      "Max depth of 7 n_estimators= 10 F1 Score = 0.5625965996908809 AUC-ROC score = 0.8427434233209734\n",
      "Max depth of 7 n_estimators= 20 F1 Score = 0.5625000000000001 AUC-ROC score = 0.8453996515826977\n",
      "Max depth of 7 n_estimators= 30 F1 Score = 0.5414012738853503 AUC-ROC score = 0.8483704232423375\n",
      "Max depth of 7 n_estimators= 40 F1 Score = 0.5500794912559619 AUC-ROC score = 0.8498758460914958\n",
      "Max depth of 7 n_estimators= 50 F1 Score = 0.560126582278481 AUC-ROC score = 0.8500542889806979\n",
      "Max depth of 7 n_estimators= 60 F1 Score = 0.5615141955835962 AUC-ROC score = 0.8515309795002389\n",
      "Max depth of 7 n_estimators= 70 F1 Score = 0.5596184419713831 AUC-ROC score = 0.8519044997852636\n",
      "Max depth of 7 n_estimators= 80 F1 Score = 0.5650793650793651 AUC-ROC score = 0.85216006629607\n",
      "Max depth of 7 n_estimators= 90 F1 Score = 0.5605095541401274 AUC-ROC score = 0.8518167905685371\n",
      "Max depth of 7 n_estimators= 100 F1 Score = 0.5596184419713831 AUC-ROC score = 0.8524821708333585\n",
      "Max depth of 8 n_estimators= 10 F1 Score = 0.5480314960629922 AUC-ROC score = 0.8387965085682833\n",
      "Max depth of 8 n_estimators= 20 F1 Score = 0.5641838351822503 AUC-ROC score = 0.8447448569129985\n",
      "Max depth of 8 n_estimators= 30 F1 Score = 0.5650793650793651 AUC-ROC score = 0.8478555096510383\n",
      "Max depth of 8 n_estimators= 40 F1 Score = 0.5654952076677316 AUC-ROC score = 0.8500353861322656\n",
      "Max depth of 8 n_estimators= 50 F1 Score = 0.5705229793977813 AUC-ROC score = 0.8499537258270373\n",
      "Max depth of 8 n_estimators= 60 F1 Score = 0.5700636942675159 AUC-ROC score = 0.8509003804765332\n",
      "Max depth of 8 n_estimators= 70 F1 Score = 0.5623003194888179 AUC-ROC score = 0.8509593573636424\n",
      "Max depth of 8 n_estimators= 80 F1 Score = 0.5632 AUC-ROC score = 0.8517169835288141\n",
      "Max depth of 8 n_estimators= 90 F1 Score = 0.5608974358974359 AUC-ROC score = 0.8523778271100115\n",
      "Max depth of 8 n_estimators= 100 F1 Score = 0.5641025641025641 AUC-ROC score = 0.8531052087176911\n",
      "Max depth of 9 n_estimators= 10 F1 Score = 0.5809379727685325 AUC-ROC score = 0.8446911728234505\n",
      "Max depth of 9 n_estimators= 20 F1 Score = 0.5763239875389408 AUC-ROC score = 0.8456257296499494\n",
      "Max depth of 9 n_estimators= 30 F1 Score = 0.5709828393135726 AUC-ROC score = 0.8474933310750729\n",
      "Max depth of 9 n_estimators= 40 F1 Score = 0.5745682888540031 AUC-ROC score = 0.8501397298556125\n",
      "Max depth of 9 n_estimators= 50 F1 Score = 0.5709779179810726 AUC-ROC score = 0.8515975175267211\n",
      "Max depth of 9 n_estimators= 60 F1 Score = 0.5611285266457681 AUC-ROC score = 0.8520587470284724\n",
      "Max depth of 9 n_estimators= 70 F1 Score = 0.555205047318612 AUC-ROC score = 0.8518561084932768\n",
      "Max depth of 9 n_estimators= 80 F1 Score = 0.5578446909667195 AUC-ROC score = 0.8514750270688789\n",
      "Max depth of 9 n_estimators= 90 F1 Score = 0.5606299212598426 AUC-ROC score = 0.8516383476793351\n",
      "Max depth of 9 n_estimators= 100 F1 Score = 0.5597484276729561 AUC-ROC score = 0.851928695431257\n",
      "Max depth of 10 n_estimators= 10 F1 Score = 0.5869894099848714 AUC-ROC score = 0.8462071812677309\n",
      "Max depth of 10 n_estimators= 20 F1 Score = 0.581039755351682 AUC-ROC score = 0.8490456329883439\n",
      "Max depth of 10 n_estimators= 30 F1 Score = 0.563944530046225 AUC-ROC score = 0.8496785003538614\n",
      "Max depth of 10 n_estimators= 40 F1 Score = 0.5612403100775193 AUC-ROC score = 0.850097387475124\n",
      "Max depth of 10 n_estimators= 50 F1 Score = 0.551937984496124 AUC-ROC score = 0.8504845178110197\n",
      "Max depth of 10 n_estimators= 60 F1 Score = 0.5718701700154559 AUC-ROC score = 0.8497843563050829\n",
      "Max depth of 10 n_estimators= 70 F1 Score = 0.5674418604651162 AUC-ROC score = 0.8509094538437809\n",
      "Max depth of 10 n_estimators= 80 F1 Score = 0.5771604938271605 AUC-ROC score = 0.8512451684319406\n",
      "Max depth of 10 n_estimators= 90 F1 Score = 0.5740740740740741 AUC-ROC score = 0.8517835215552961\n",
      "Max depth of 10 n_estimators= 100 F1 Score = 0.5692068429237946 AUC-ROC score = 0.8520829426744657\n"
     ]
    }
   ],
   "source": [
    "#create a loop to test max_depth and n_estimators\n",
    "for depth in range (6,11):\n",
    "    for est in range (10, 101, 10):\n",
    "        forest_model = RandomForestClassifier(random_state = 12345, max_depth = depth, n_estimators = est)\n",
    "        forest_model.fit(features_train, target_train)\n",
    "        forest_prediction = forest_model.predict(features_valid)\n",
    "        forst_f1 = f1_score(target_valid, forest_prediction)\n",
    "        forest_valid = forest_model.predict_proba(features_valid)\n",
    "        forest_1_valid = forest_valid[:,1]\n",
    "        print('Max depth of', depth, 'n_estimators=', est, 'F1 Score =', f1_score(target_valid, forest_prediction), 'AUC-ROC score =', roc_auc_score(target_valid, forest_1_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best model with max depth of 10, n_estimator = 10 give an F1 score of ~0.59, AUC-ROC score of ~0.85."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do for Logistic Regression is set a solver, we will use the 'liblinear'. and keep the random_state the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score = 0.08385744234800838 AUC-ROC score = 0.6727947180904797\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "logistic_model = LogisticRegression(random_state = 12345, solver = 'liblinear')\n",
    "logistic_model.fit(features_train, target_train)\n",
    "logistic_prediction = logistic_model.predict(features_valid)\n",
    "logistic_valid = logistic_model.predict_proba(features_valid)\n",
    "logistic_1_valid = logistic_valid[:,1]\n",
    "print('F1 Score =', f1_score(target_valid, logistic_prediction), 'AUC-ROC score =', roc_auc_score(target_valid, logistic_1_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomforest model with max-depth = 10, n_estimator = 10 gave the highest F1 score (~0.59) follow by decision tree (max_depth of 6), F1 = ~0.57. Logistic Regression Model gave the lowest F1 score (0.33)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no definite range of our numerical columns, it will be best to scale them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CreditScore       Age    Tenure   Balance  NumOfProducts  HasCrCard  \\\n",
      "7479    -0.886751 -0.373192  1.082277  1.232271      -0.891560          1   \n",
      "3411     0.608663 -0.183385  1.082277  0.600563      -0.891560          0   \n",
      "6027     2.052152  0.480939 -0.737696  1.027098       0.830152          0   \n",
      "1247    -1.457915 -1.417129  0.354288 -1.233163       0.830152          1   \n",
      "3716     0.130961 -1.132419 -1.101690  1.140475      -0.891560          0   \n",
      "\n",
      "      IsActiveMember  EstimatedSalary  Geography_Germany  Geography_Spain  \\\n",
      "7479               0        -0.187705                  0                1   \n",
      "3411               0        -0.333945                  0                0   \n",
      "6027               1         1.503095                  1                0   \n",
      "1247               0        -1.071061                  0                0   \n",
      "3716               0         1.524268                  1                0   \n",
      "\n",
      "      Gender_Male  \n",
      "7479            1  \n",
      "3411            0  \n",
      "6027            1  \n",
      "1247            1  \n",
      "3716            0  \n",
      "      CreditScore       Age    Tenure   Balance  NumOfProducts  HasCrCard  \\\n",
      "8532    -0.699824 -0.373192 -1.101690 -1.233163       0.830152          1   \n",
      "5799    -0.284431  0.575842 -0.737696 -1.233163      -0.891560          1   \n",
      "5511     0.151731 -0.657902 -1.829679  0.438711      -0.891560          1   \n",
      "7365    -0.876366 -0.278288  1.810266  1.239884      -0.891560          1   \n",
      "7367    -0.481743  0.291132  1.810266 -1.233163       0.830152          1   \n",
      "\n",
      "      IsActiveMember  EstimatedSalary  Geography_Germany  Geography_Spain  \\\n",
      "8532               0        -0.015173                  0                0   \n",
      "5799               1         1.471724                  0                0   \n",
      "5511               0        -1.367107                  1                0   \n",
      "7365               1        -0.786517                  0                1   \n",
      "7367               0         1.358533                  0                1   \n",
      "\n",
      "      Gender_Male  \n",
      "8532            0  \n",
      "5799            0  \n",
      "5511            1  \n",
      "7365            0  \n",
      "7367            1  \n",
      "      CreditScore       Age    Tenure   Balance  NumOfProducts  HasCrCard  \\\n",
      "7041    -2.226392 -0.088482 -1.101690 -1.233163       0.830152          1   \n",
      "5709    -0.087120  0.006422  1.446272 -1.233163      -0.891560          1   \n",
      "7117    -0.917905 -0.752805 -0.009707  0.722307      -0.891560          1   \n",
      "7775    -0.253277  0.101325  1.810266 -1.233163       0.830152          1   \n",
      "8735     0.785204 -0.847708  1.810266  0.615625      -0.891560          0   \n",
      "\n",
      "      IsActiveMember  EstimatedSalary  Geography_Germany  Geography_Spain  \\\n",
      "7041               0         0.647083                  0                0   \n",
      "5709               0        -1.658410                  0                0   \n",
      "7117               1        -1.369334                  0                1   \n",
      "7775               0         0.075086                  0                1   \n",
      "8735               1        -1.070919                  0                0   \n",
      "\n",
      "      Gender_Male  \n",
      "7041            1  \n",
      "5709            0  \n",
      "7117            1  \n",
      "7775            1  \n",
      "8735            1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3890043125.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features_train[numerical] = scaler.transform(features_train[numerical])\n",
      "/opt/conda/lib/python3.9/site-packages/pandas/core/indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "/tmp/ipykernel_31/3890043125.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features_valid[numerical] = scaler.transform(features_valid[numerical])\n",
      "/opt/conda/lib/python3.9/site-packages/pandas/core/indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "/tmp/ipykernel_31/3890043125.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  features_test[numerical] = scaler.transform(features_test[numerical])\n",
      "/opt/conda/lib/python3.9/site-packages/pandas/core/indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    }
   ],
   "source": [
    "#scaling\n",
    "\n",
    "numerical = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(features_train[numerical])\n",
    "\n",
    "features_train[numerical] = scaler.transform(features_train[numerical])\n",
    "features_valid[numerical] = scaler.transform(features_valid[numerical])\n",
    "features_test[numerical] = scaler.transform(features_test[numerical])\n",
    "\n",
    "print(features_train.head())\n",
    "print(features_valid.head())\n",
    "print(features_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those numerical columns should be properly scaled now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weight adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by adjusting the class weight in the hyperparameter. Since we saw that RandomForest gave the highest F1, let's improve on this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score = 0.6038647342995168 AUC-ROC score = 0.8378377560957906\n"
     ]
    }
   ],
   "source": [
    "#improve model by adjusting class weight\n",
    "\n",
    "adj_forest_model = RandomForestClassifier(random_state = 12345, max_depth = 10, n_estimators=10, class_weight = 'balanced')\n",
    "adj_forest_model.fit(features_train, target_train)\n",
    "adj_forest_pred = adj_forest_model.predict(features_valid)\n",
    "adj_forest_valid = adj_forest_model.predict_proba(features_valid)\n",
    "adj_forest_1_valid = adj_forest_valid[:,1]\n",
    "print('F1 Score =', f1_score(target_valid, adj_forest_pred), 'AUC-ROC score =', roc_auc_score(target_valid, adj_forest_1_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our F1 score has improved, but AUC-ROC decreased due to the True Positive Rate decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling/Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not know which method gives the best result, let's try them both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def a function for upsampling\n",
    "\n",
    "def upsample(features, target, repeat):\n",
    "    \n",
    "    #split samples into negative and positive observations\n",
    "    features_zero = features[target == 0]\n",
    "    features_one = features[target == 1]\n",
    "    target_zero = target[target == 0]\n",
    "    target_one = target[target == 1]\n",
    "    \n",
    "    #duplicate positive class and combine with negative class\n",
    "    features_upsampled = pd.concat([features_zero] + [features_one] * repeat)\n",
    "    target_upsampled = pd.concat([target_zero] + [target_one] * repeat)\n",
    "    \n",
    "    #shuffle data\n",
    "    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state = 12345)\n",
    "    \n",
    "    #return the upsampled target and feature\n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsampling with 1 repeats:\n",
      "F1 Score = 0.5635528330781011\n",
      "Upsampling with 2 repeats:\n",
      "F1 Score = 0.6274007682458387\n",
      "Upsampling with 3 repeats:\n",
      "F1 Score = 0.5986238532110092\n",
      "Upsampling with 4 repeats:\n",
      "F1 Score = 0.5849462365591399\n",
      "Upsampling with 5 repeats:\n",
      "F1 Score = 0.6012396694214877\n"
     ]
    }
   ],
   "source": [
    "#test the data\n",
    "\n",
    "for i in range(1, 6): \n",
    "    features_train_upsampled, target_train_upsampled = upsample(features_train, target_train, i)\n",
    "    model = RandomForestClassifier(random_state = 12345, max_depth = 10, n_estimators = 10)\n",
    "    model.fit(features_train_upsampled, target_train_upsampled)\n",
    "    prediction_valid = model.predict(features_valid)\n",
    "    print('Upsampling with', i, 'repeats:\\n''F1 Score =', f1_score(target_valid, prediction_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, upsampling with 2 repeats gave us the best F1 score, F1 = ~0.62. Now let's see what downsampling can do for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function for downsampling\n",
    "\n",
    "\n",
    "def downsample(features, target, fraction):\n",
    "    \n",
    "    #split samples into negative and positive observations\n",
    "    features_zero = features[target == 0]\n",
    "    features_one = features[target == 1]\n",
    "    target_zero = target[target == 0]\n",
    "    target_one = target[target == 1]\n",
    "    \n",
    "    #take fractions of negative class and combine with positive class\n",
    "    features_downsampled = pd.concat([features_zero.sample(frac=fraction, random_state =12345)] + [features_one])\n",
    "    target_downsampled = pd.concat([target_zero.sample(frac=fraction, random_state = 12345)] + [target_one])\n",
    "    \n",
    "    #shuffle data\n",
    "    features_downsampled, target_downsampled = shuffle(features_downsampled, target_downsampled, random_state = 12345)\n",
    "    \n",
    "    #return the upsampled target and feature\n",
    "    return features_downsampled, target_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampling with fraction of 0.1 :\n",
      "F1 Score = 0.45058317986494784\n",
      "downsampling with fraction of 0.2 :\n",
      "F1 Score = 0.5595854922279794\n",
      "downsampling with fraction of 0.30000000000000004 :\n",
      "F1 Score = 0.5938775510204082\n",
      "downsampling with fraction of 0.4 :\n",
      "F1 Score = 0.6255707762557078\n",
      "downsampling with fraction of 0.5 :\n",
      "F1 Score = 0.6134663341645886\n",
      "downsampling with fraction of 0.6 :\n",
      "F1 Score = 0.6271870794078062\n",
      "downsampling with fraction of 0.7000000000000001 :\n",
      "F1 Score = 0.5963431786216596\n",
      "downsampling with fraction of 0.8 :\n",
      "F1 Score = 0.5872093023255813\n",
      "downsampling with fraction of 0.9 :\n",
      "F1 Score = 0.575301204819277\n"
     ]
    }
   ],
   "source": [
    "#test the data\n",
    "\n",
    "for i in np.arange(0.1, 1.0, 0.1): \n",
    "    features_train_downsampled, target_train_downsampled = downsample(features_train, target_train, i)\n",
    "    model = RandomForestClassifier(random_state = 12345, max_depth = 10, n_estimators = 10)\n",
    "    model.fit(features_train_downsampled, target_train_downsampled)\n",
    "    prediction_valid = model.predict(features_valid)\n",
    "    print('downsampling with fraction of', i, ':\\n''F1 Score =', f1_score(target_valid, prediction_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appeared that downsampling with fraction of 0.6 gave us the best f1 score (~0.63). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to combine both up & down sampling methods to see whether this will give us even higher F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will do both up and down sampling.\n",
    "\n",
    "def setsample(features, target, fraction, repeat):\n",
    "    \n",
    "    #split samples into negative and positive observations\n",
    "    features_zero = features[target == 0]\n",
    "    features_one = features[target == 1]\n",
    "    target_zero = target[target == 0]\n",
    "    target_one = target[target == 1]\n",
    "    \n",
    "    #take fractions of negative class and combine with positive class * repeat\n",
    "    features_sampled = pd.concat([features_zero.sample(frac=fraction, random_state =12345)] + [features_one] * repeat)\n",
    "    target_sampled = pd.concat([target_zero.sample(frac=fraction, random_state = 12345)] + [target_one] * repeat)\n",
    "    \n",
    "    #shuffle data\n",
    "    features_sampled, target_sampled = shuffle(features_sampled, target_sampled, random_state = 12345)\n",
    "    \n",
    "    #return the upsampled target and feature\n",
    "    return features_sampled, target_sampled\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction = 0.1 repeat = 1 F1 Score = 0.45058317986494784\n",
      "fraction = 0.1 repeat = 2 F1 Score = 0.4296619411123228\n",
      "fraction = 0.1 repeat = 3 F1 Score = 0.40346408558329094\n",
      "fraction = 0.1 repeat = 4 F1 Score = 0.4001991040318566\n",
      "fraction = 0.1 repeat = 5 F1 Score = 0.40383644623927306\n",
      "fraction = 0.2 repeat = 1 F1 Score = 0.5595854922279794\n",
      "fraction = 0.2 repeat = 2 F1 Score = 0.49348869088416725\n",
      "fraction = 0.2 repeat = 3 F1 Score = 0.46840148698884765\n",
      "fraction = 0.2 repeat = 4 F1 Score = 0.4564564564564565\n",
      "fraction = 0.2 repeat = 5 F1 Score = 0.4317425083240843\n",
      "fraction = 0.30000000000000004 repeat = 1 F1 Score = 0.5938775510204082\n",
      "fraction = 0.30000000000000004 repeat = 2 F1 Score = 0.5339805825242719\n",
      "fraction = 0.30000000000000004 repeat = 3 F1 Score = 0.5075921908893709\n",
      "fraction = 0.30000000000000004 repeat = 4 F1 Score = 0.48823133826496307\n",
      "fraction = 0.30000000000000004 repeat = 5 F1 Score = 0.47912652536929995\n",
      "fraction = 0.4 repeat = 1 F1 Score = 0.6255707762557078\n",
      "fraction = 0.4 repeat = 2 F1 Score = 0.5642458100558659\n",
      "fraction = 0.4 repeat = 3 F1 Score = 0.5529122231337161\n",
      "fraction = 0.4 repeat = 4 F1 Score = 0.5323076923076923\n",
      "fraction = 0.4 repeat = 5 F1 Score = 0.5101156069364162\n",
      "fraction = 0.5 repeat = 1 F1 Score = 0.6134663341645886\n",
      "fraction = 0.5 repeat = 2 F1 Score = 0.5743174924165824\n",
      "fraction = 0.5 repeat = 3 F1 Score = 0.5683060109289617\n",
      "fraction = 0.5 repeat = 4 F1 Score = 0.5621351125938282\n",
      "fraction = 0.5 repeat = 5 F1 Score = 0.5397324940991346\n",
      "fraction = 0.6 repeat = 1 F1 Score = 0.6271870794078062\n",
      "fraction = 0.6 repeat = 2 F1 Score = 0.5849889624724062\n",
      "fraction = 0.6 repeat = 3 F1 Score = 0.5657764589515333\n",
      "fraction = 0.6 repeat = 4 F1 Score = 0.5620567375886525\n",
      "fraction = 0.6 repeat = 5 F1 Score = 0.5439469320066336\n",
      "fraction = 0.7000000000000001 repeat = 1 F1 Score = 0.5963431786216596\n",
      "fraction = 0.7000000000000001 repeat = 2 F1 Score = 0.6036866359447004\n",
      "fraction = 0.7000000000000001 repeat = 3 F1 Score = 0.602308499475341\n",
      "fraction = 0.7000000000000001 repeat = 4 F1 Score = 0.5727969348659004\n",
      "fraction = 0.7000000000000001 repeat = 5 F1 Score = 0.5585585585585586\n",
      "fraction = 0.8 repeat = 1 F1 Score = 0.5872093023255813\n",
      "fraction = 0.8 repeat = 2 F1 Score = 0.6096654275092936\n",
      "fraction = 0.8 repeat = 3 F1 Score = 0.6047008547008547\n",
      "fraction = 0.8 repeat = 4 F1 Score = 0.5958702064896755\n",
      "fraction = 0.8 repeat = 5 F1 Score = 0.5695732838589982\n",
      "fraction = 0.9 repeat = 1 F1 Score = 0.575301204819277\n",
      "fraction = 0.9 repeat = 2 F1 Score = 0.6201550387596899\n",
      "fraction = 0.9 repeat = 3 F1 Score = 0.6096997690531178\n",
      "fraction = 0.9 repeat = 4 F1 Score = 0.6077003121748179\n",
      "fraction = 0.9 repeat = 5 F1 Score = 0.5842259006815969\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "for fraction in np.arange(0.1, 1.0, 0.1):\n",
    "    for i in range (1, 6):\n",
    "        features_train_sampled, target_train_sampled = setsample(features_train, target_train, fraction, i)\n",
    "        model = RandomForestClassifier(random_state = 12345, max_depth = 10, n_estimators = 10)\n",
    "        model.fit(features_train_sampled, target_train_sampled)\n",
    "        prediction_valid = model.predict(features_valid)\n",
    "        print('fraction =', fraction, 'repeat =', i, 'F1 Score =', f1_score(target_valid, prediction_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining both up-and down-sampling did not really give us a better result. Fraction of 0.6, repeat = 1 still resulted in the highest F1 score (~0.63)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score = 0.6139534883720931\n"
     ]
    }
   ],
   "source": [
    "#what if we combined both methods (class balance and up/down sampling) together\n",
    "\n",
    "features_train_sampled, target_train_sampled = setsample(features_train, target_train, 0.6, 1)\n",
    "model = RandomForestClassifier(random_state = 12345, max_depth = 10, n_estimators = 10, class_weight = 'balanced')\n",
    "model.fit(features_train_sampled, target_train_sampled)\n",
    "prediction_valid = model.predict(features_valid)\n",
    "print('F1 Score =', f1_score(target_valid, prediction_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appeared that using the combination of fraction of 0.6, and repeat of 1 will grant us the highest F1 score (~0.63)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score = 0.5973451327433629\n",
      "AUC-ROC Score = 0.835636686349729\n"
     ]
    }
   ],
   "source": [
    "# Let's first combined the train and valid set.\n",
    "\n",
    "features = pd.concat([features_train, features_valid])\n",
    "target = pd.concat([target_train, target_valid])\n",
    "\n",
    "# Test our model\n",
    "features_sampled, target_sampled = setsample(features, target, 0.6, 1)\n",
    "model = RandomForestClassifier(random_state = 12345, max_depth = 10, n_estimators = 10, class_weight='balanced')\n",
    "model.fit(features_sampled, target_sampled)\n",
    "prediction_valid = model.predict(features_test)\n",
    "probabilities_valid = model.predict_proba(features_test)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "print('F1 Score =', f1_score(target_test, prediction_valid))\n",
    "print('AUC-ROC Score =', roc_auc_score(target_test, probabilities_one_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using both up/downsampling and balancing the class weight, we acheived ~0.6 for our final F1 score. Our AUC-ROC score became ~0.84, showing good separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to visualize\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(target_test, probabilities_one_valid)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1],[0,1], linestyle = '--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We processed the dataset by first scaling numeric columns, then we fill in missing values, and finally set dummy columns from categorical columns. \n",
    "\n",
    "We then split the data into train, valid, and test sets following a 3:1:1 ratio. \n",
    "\n",
    "Although we saw an approximate 4:1 class imbalance, we trained Decision Tree, Random Forest, and Logistic Regression Classifier models with class imblanced. Random Forest appeared to be the best model with the highest f1 score (about 0.59) and AUC-ROC value of ~0.85. \n",
    "\n",
    "We then took the class imbalance into consideration and used to two approaches to address: Class weight adjustment and a combination of up and down sampling. We trained our final model with the combination of both methods, the F1 score for our final model on test set is ~0.6, whihle AUC-ROC is ~0.84."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 988,
    "start_time": "2022-06-29T16:10:37.634Z"
   },
   {
    "duration": 74,
    "start_time": "2022-06-29T16:12:01.221Z"
   },
   {
    "duration": 5,
    "start_time": "2022-06-29T16:23:43.782Z"
   },
   {
    "duration": 84,
    "start_time": "2022-06-29T16:25:17.653Z"
   },
   {
    "duration": 5,
    "start_time": "2022-06-29T16:25:26.251Z"
   },
   {
    "duration": 12,
    "start_time": "2022-06-29T16:25:53.017Z"
   },
   {
    "duration": 12,
    "start_time": "2022-06-29T16:28:08.260Z"
   },
   {
    "duration": 18,
    "start_time": "2022-06-29T16:32:55.396Z"
   },
   {
    "duration": 25,
    "start_time": "2022-06-29T16:40:50.365Z"
   },
   {
    "duration": 28,
    "start_time": "2022-06-29T16:41:26.606Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-29T16:41:38.857Z"
   },
   {
    "duration": 41,
    "start_time": "2022-06-29T16:41:38.865Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-29T16:41:38.908Z"
   },
   {
    "duration": 15,
    "start_time": "2022-06-29T16:41:38.914Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-29T16:41:38.931Z"
   },
   {
    "duration": 19,
    "start_time": "2022-06-29T16:41:38.944Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-29T16:41:41.054Z"
   },
   {
    "duration": 42,
    "start_time": "2022-06-29T16:41:41.061Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-29T16:41:41.105Z"
   },
   {
    "duration": 13,
    "start_time": "2022-06-29T16:41:41.109Z"
   },
   {
    "duration": 9,
    "start_time": "2022-06-29T16:41:41.123Z"
   },
   {
    "duration": 23,
    "start_time": "2022-06-29T16:41:41.134Z"
   },
   {
    "duration": 23,
    "start_time": "2022-06-29T16:41:41.158Z"
   },
   {
    "duration": 9,
    "start_time": "2022-06-29T16:45:07.824Z"
   },
   {
    "duration": 9,
    "start_time": "2022-06-29T16:48:24.164Z"
   },
   {
    "duration": 22,
    "start_time": "2022-06-29T16:55:13.693Z"
   },
   {
    "duration": 187,
    "start_time": "2022-06-29T16:56:45.550Z"
   },
   {
    "duration": 5290,
    "start_time": "2022-06-29T17:05:00.974Z"
   },
   {
    "duration": 31,
    "start_time": "2022-06-29T17:07:08.465Z"
   },
   {
    "duration": 34,
    "start_time": "2022-06-29T17:09:11.302Z"
   },
   {
    "duration": 36,
    "start_time": "2022-06-29T17:11:30.352Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-29T17:13:08.150Z"
   },
   {
    "duration": 44,
    "start_time": "2022-06-29T17:13:08.155Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-29T17:13:08.201Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-29T17:13:08.207Z"
   },
   {
    "duration": 10,
    "start_time": "2022-06-29T17:13:08.219Z"
   },
   {
    "duration": 15,
    "start_time": "2022-06-29T17:13:08.316Z"
   },
   {
    "duration": 25,
    "start_time": "2022-06-29T17:13:08.333Z"
   },
   {
    "duration": 10,
    "start_time": "2022-06-29T17:13:08.359Z"
   },
   {
    "duration": 5,
    "start_time": "2022-06-29T17:13:08.370Z"
   },
   {
    "duration": 173,
    "start_time": "2022-06-29T17:13:08.478Z"
   },
   {
    "duration": 21121,
    "start_time": "2022-06-29T17:13:08.653Z"
   },
   {
    "duration": 511,
    "start_time": "2022-06-29T17:17:14.601Z"
   },
   {
    "duration": 24358,
    "start_time": "2022-06-29T17:18:21.722Z"
   },
   {
    "duration": 25853,
    "start_time": "2022-06-29T17:19:37.094Z"
   },
   {
    "duration": 26420,
    "start_time": "2022-06-29T17:20:56.223Z"
   },
   {
    "duration": 26061,
    "start_time": "2022-06-29T17:21:36.297Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-29T17:22:51.054Z"
   },
   {
    "duration": 48,
    "start_time": "2022-06-29T17:22:51.059Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-29T17:22:51.109Z"
   },
   {
    "duration": 15,
    "start_time": "2022-06-29T17:22:51.114Z"
   },
   {
    "duration": 22,
    "start_time": "2022-06-29T17:22:51.132Z"
   },
   {
    "duration": 18,
    "start_time": "2022-06-29T17:22:51.156Z"
   },
   {
    "duration": 27,
    "start_time": "2022-06-29T17:22:51.214Z"
   },
   {
    "duration": 9,
    "start_time": "2022-06-29T17:22:51.243Z"
   },
   {
    "duration": 5,
    "start_time": "2022-06-29T17:22:51.254Z"
   },
   {
    "duration": 231,
    "start_time": "2022-06-29T17:22:51.260Z"
   },
   {
    "duration": 24617,
    "start_time": "2022-06-29T17:22:51.493Z"
   },
   {
    "duration": 25108,
    "start_time": "2022-06-29T17:23:58.107Z"
   },
   {
    "duration": 23613,
    "start_time": "2022-06-29T17:25:39.469Z"
   },
   {
    "duration": 22720,
    "start_time": "2022-06-29T17:26:18.524Z"
   },
   {
    "duration": 21949,
    "start_time": "2022-06-29T17:27:12.953Z"
   },
   {
    "duration": 22566,
    "start_time": "2022-06-29T17:27:46.094Z"
   },
   {
    "duration": 44,
    "start_time": "2022-06-29T17:29:12.023Z"
   },
   {
    "duration": 22830,
    "start_time": "2022-06-29T17:29:28.179Z"
   },
   {
    "duration": 15826,
    "start_time": "2022-06-29T17:30:19.538Z"
   },
   {
    "duration": 13939,
    "start_time": "2022-06-29T17:31:09.090Z"
   },
   {
    "duration": 24,
    "start_time": "2022-06-29T17:36:27.068Z"
   },
   {
    "duration": 95,
    "start_time": "2022-06-29T17:44:59.449Z"
   },
   {
    "duration": 8,
    "start_time": "2022-06-29T17:46:27.734Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-29T17:55:27.345Z"
   },
   {
    "duration": 18,
    "start_time": "2022-06-29T17:59:29.236Z"
   },
   {
    "duration": 5,
    "start_time": "2022-06-29T17:59:39.639Z"
   },
   {
    "duration": 377,
    "start_time": "2022-06-29T17:59:42.090Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-29T18:04:38.409Z"
   },
   {
    "duration": 10,
    "start_time": "2022-06-29T18:05:53.853Z"
   },
   {
    "duration": 435,
    "start_time": "2022-06-29T18:06:02.153Z"
   },
   {
    "duration": 428,
    "start_time": "2022-06-29T18:06:56.043Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-29T18:10:22.920Z"
   },
   {
    "duration": 18,
    "start_time": "2022-06-29T18:17:12.996Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-29T18:19:50.281Z"
   },
   {
    "duration": 8,
    "start_time": "2022-06-29T18:21:36.150Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-29T18:21:40.298Z"
   },
   {
    "duration": 8,
    "start_time": "2022-06-29T18:21:42.420Z"
   },
   {
    "duration": 72,
    "start_time": "2022-06-29T18:22:14.400Z"
   },
   {
    "duration": 2397,
    "start_time": "2022-06-29T18:24:40.177Z"
   },
   {
    "duration": 2454,
    "start_time": "2022-06-29T18:25:29.637Z"
   },
   {
    "duration": 2295,
    "start_time": "2022-06-29T18:25:37.977Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-29T18:25:46.286Z"
   },
   {
    "duration": 2381,
    "start_time": "2022-06-29T18:25:48.241Z"
   },
   {
    "duration": 2525,
    "start_time": "2022-06-29T18:27:27.739Z"
   },
   {
    "duration": 55,
    "start_time": "2022-06-29T18:32:19.037Z"
   },
   {
    "duration": 65,
    "start_time": "2022-06-29T18:33:36.780Z"
   },
   {
    "duration": 80,
    "start_time": "2022-06-29T18:37:25.267Z"
   },
   {
    "duration": 78,
    "start_time": "2022-06-29T18:38:24.670Z"
   },
   {
    "duration": 77,
    "start_time": "2022-06-29T18:38:31.675Z"
   },
   {
    "duration": 86,
    "start_time": "2022-06-29T18:38:37.948Z"
   },
   {
    "duration": 73,
    "start_time": "2022-06-29T18:38:48.215Z"
   },
   {
    "duration": 82,
    "start_time": "2022-06-29T18:39:03.974Z"
   },
   {
    "duration": 68,
    "start_time": "2022-06-29T18:39:36.535Z"
   },
   {
    "duration": 67,
    "start_time": "2022-06-29T18:40:20.839Z"
   },
   {
    "duration": 91,
    "start_time": "2022-06-29T18:40:27.487Z"
   },
   {
    "duration": 78,
    "start_time": "2022-06-29T18:41:15.056Z"
   },
   {
    "duration": 80,
    "start_time": "2022-06-29T18:43:02.775Z"
   },
   {
    "duration": 47,
    "start_time": "2022-06-30T21:39:00.763Z"
   },
   {
    "duration": 1125,
    "start_time": "2022-06-30T21:39:05.149Z"
   },
   {
    "duration": 81,
    "start_time": "2022-06-30T21:39:06.276Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-30T21:39:06.359Z"
   },
   {
    "duration": 16,
    "start_time": "2022-06-30T21:39:06.365Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-30T21:39:06.383Z"
   },
   {
    "duration": 31,
    "start_time": "2022-06-30T21:39:06.396Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-30T21:39:06.429Z"
   },
   {
    "duration": 7,
    "start_time": "2022-06-30T21:39:06.442Z"
   },
   {
    "duration": 250,
    "start_time": "2022-06-30T21:39:06.451Z"
   },
   {
    "duration": 14702,
    "start_time": "2022-06-30T21:39:06.704Z"
   },
   {
    "duration": 45,
    "start_time": "2022-06-30T21:39:21.407Z"
   },
   {
    "duration": 163,
    "start_time": "2022-06-30T21:39:21.529Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-30T21:39:21.694Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-30T21:39:21.694Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-30T21:39:21.695Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-30T21:39:21.697Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-30T21:39:21.698Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-30T21:39:21.700Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-30T21:39:21.722Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-30T21:39:21.724Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-30T21:39:21.725Z"
   },
   {
    "duration": 33,
    "start_time": "2022-06-30T21:39:43.229Z"
   },
   {
    "duration": 33,
    "start_time": "2022-06-30T21:39:54.392Z"
   },
   {
    "duration": 31,
    "start_time": "2022-06-30T21:40:09.320Z"
   },
   {
    "duration": 73,
    "start_time": "2022-06-30T21:40:44.058Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-30T21:41:06.318Z"
   },
   {
    "duration": 387,
    "start_time": "2022-06-30T21:41:09.237Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-30T21:41:13.776Z"
   },
   {
    "duration": 446,
    "start_time": "2022-06-30T21:41:16.205Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-30T21:41:20.310Z"
   },
   {
    "duration": 2912,
    "start_time": "2022-06-30T21:41:22.916Z"
   },
   {
    "duration": 60,
    "start_time": "2022-06-30T21:41:28.544Z"
   },
   {
    "duration": 81,
    "start_time": "2022-06-30T21:41:32.435Z"
   },
   {
    "duration": 28,
    "start_time": "2022-06-30T21:42:39.161Z"
   },
   {
    "duration": 37,
    "start_time": "2022-06-30T21:43:44.907Z"
   },
   {
    "duration": 41,
    "start_time": "2022-06-30T21:44:21.707Z"
   },
   {
    "duration": 66,
    "start_time": "2022-06-30T21:44:30.477Z"
   },
   {
    "duration": 38,
    "start_time": "2022-06-30T21:45:54.103Z"
   },
   {
    "duration": 74,
    "start_time": "2022-06-30T21:46:06.138Z"
   },
   {
    "duration": 28,
    "start_time": "2022-06-30T21:46:28.115Z"
   },
   {
    "duration": 29,
    "start_time": "2022-06-30T21:46:54.856Z"
   },
   {
    "duration": 31,
    "start_time": "2022-06-30T21:47:10.045Z"
   },
   {
    "duration": 30,
    "start_time": "2022-06-30T21:47:25.196Z"
   },
   {
    "duration": 27,
    "start_time": "2022-06-30T21:47:45.340Z"
   },
   {
    "duration": 38,
    "start_time": "2022-06-30T21:48:50.247Z"
   },
   {
    "duration": 40,
    "start_time": "2022-06-30T21:48:59.575Z"
   },
   {
    "duration": 74,
    "start_time": "2022-06-30T21:49:04.334Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-30T21:49:14.401Z"
   },
   {
    "duration": 394,
    "start_time": "2022-06-30T21:49:16.894Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-30T21:49:31.841Z"
   },
   {
    "duration": 501,
    "start_time": "2022-06-30T21:49:34.216Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-30T21:49:46.925Z"
   },
   {
    "duration": 2815,
    "start_time": "2022-06-30T21:49:49.271Z"
   },
   {
    "duration": 63,
    "start_time": "2022-06-30T21:50:04.108Z"
   },
   {
    "duration": 79,
    "start_time": "2022-06-30T21:50:12.127Z"
   },
   {
    "duration": 78,
    "start_time": "2022-06-30T21:50:29.377Z"
   },
   {
    "duration": 80,
    "start_time": "2022-06-30T21:50:36.285Z"
   },
   {
    "duration": 65,
    "start_time": "2022-06-30T21:53:43.155Z"
   },
   {
    "duration": 13,
    "start_time": "2022-06-30T21:55:57.836Z"
   },
   {
    "duration": 10,
    "start_time": "2022-06-30T21:58:17.858Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-30T21:58:46.051Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-30T21:58:50.603Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-30T21:59:10.860Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-30T21:59:50.176Z"
   },
   {
    "duration": 309,
    "start_time": "2022-06-30T22:02:16.925Z"
   },
   {
    "duration": 168,
    "start_time": "2022-06-30T22:02:22.319Z"
   },
   {
    "duration": 111,
    "start_time": "2022-06-30T22:02:36.399Z"
   },
   {
    "duration": 5,
    "start_time": "2022-06-30T22:03:08.688Z"
   },
   {
    "duration": 72,
    "start_time": "2022-06-30T22:03:08.695Z"
   },
   {
    "duration": 5,
    "start_time": "2022-06-30T22:03:08.769Z"
   },
   {
    "duration": 34,
    "start_time": "2022-06-30T22:03:08.775Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-30T22:03:08.811Z"
   },
   {
    "duration": 34,
    "start_time": "2022-06-30T22:03:08.824Z"
   },
   {
    "duration": 20,
    "start_time": "2022-06-30T22:03:08.859Z"
   },
   {
    "duration": 33,
    "start_time": "2022-06-30T22:03:08.881Z"
   },
   {
    "duration": 228,
    "start_time": "2022-06-30T22:03:08.915Z"
   },
   {
    "duration": 14405,
    "start_time": "2022-06-30T22:03:09.146Z"
   },
   {
    "duration": 78,
    "start_time": "2022-06-30T22:03:23.553Z"
   },
   {
    "duration": 127,
    "start_time": "2022-06-30T22:03:23.633Z"
   },
   {
    "duration": 90,
    "start_time": "2022-06-30T22:03:23.761Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-30T22:03:23.853Z"
   },
   {
    "duration": 486,
    "start_time": "2022-06-30T22:03:23.859Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-30T22:03:24.347Z"
   },
   {
    "duration": 461,
    "start_time": "2022-06-30T22:03:24.352Z"
   },
   {
    "duration": 10,
    "start_time": "2022-06-30T22:03:24.814Z"
   },
   {
    "duration": 2899,
    "start_time": "2022-06-30T22:03:24.825Z"
   },
   {
    "duration": 58,
    "start_time": "2022-06-30T22:03:27.725Z"
   },
   {
    "duration": 96,
    "start_time": "2022-06-30T22:03:27.785Z"
   },
   {
    "duration": 116,
    "start_time": "2022-06-30T22:03:27.882Z"
   },
   {
    "duration": 79,
    "start_time": "2022-06-30T22:04:23.010Z"
   },
   {
    "duration": 1689,
    "start_time": "2022-07-01T13:00:54.401Z"
   },
   {
    "duration": 90,
    "start_time": "2022-07-01T13:00:56.093Z"
   },
   {
    "duration": 5,
    "start_time": "2022-07-01T13:00:56.185Z"
   },
   {
    "duration": 23,
    "start_time": "2022-07-01T13:00:56.193Z"
   },
   {
    "duration": 18,
    "start_time": "2022-07-01T13:00:56.218Z"
   },
   {
    "duration": 23,
    "start_time": "2022-07-01T13:00:56.238Z"
   },
   {
    "duration": 16,
    "start_time": "2022-07-01T13:00:56.263Z"
   },
   {
    "duration": 9,
    "start_time": "2022-07-01T13:00:56.280Z"
   },
   {
    "duration": 272,
    "start_time": "2022-07-01T13:00:56.292Z"
   },
   {
    "duration": 16107,
    "start_time": "2022-07-01T13:00:56.567Z"
   },
   {
    "duration": 151,
    "start_time": "2022-07-01T13:01:12.675Z"
   },
   {
    "duration": 135,
    "start_time": "2022-07-01T13:01:12.828Z"
   },
   {
    "duration": 94,
    "start_time": "2022-07-01T13:01:12.965Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-01T13:01:13.061Z"
   },
   {
    "duration": 467,
    "start_time": "2022-07-01T13:01:13.067Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-01T13:01:13.536Z"
   },
   {
    "duration": 525,
    "start_time": "2022-07-01T13:01:13.542Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-01T13:01:14.069Z"
   },
   {
    "duration": 3297,
    "start_time": "2022-07-01T13:01:14.075Z"
   },
   {
    "duration": 75,
    "start_time": "2022-07-01T13:01:17.374Z"
   },
   {
    "duration": 100,
    "start_time": "2022-07-01T13:01:17.450Z"
   },
   {
    "duration": 140,
    "start_time": "2022-07-01T13:01:17.551Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
